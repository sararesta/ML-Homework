{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "547a6093",
   "metadata": {},
   "source": [
    "# 2. Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82fa7b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code to make Jupyter print every\n",
    "# printable statement and not just the last one\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# To visualize the data\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generic libraries\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Regression models\n",
    "import sklearn\n",
    "import scipy\n",
    "from scipy.stats import *\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV #split the data into training and test\n",
    "from sklearn.linear_model import LinearRegression #linear regression\n",
    "from sklearn.preprocessing import PolynomialFeatures #for polynomial regression\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# 5-folds crossvalidation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer, classification_report, roc_auc_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb4a7cb",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37d14762",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'online_shoppers_intention.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11688\\1871395348.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtraining_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"online_shoppers_intention.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#test_set = pd.read_csv(\"test_set_complete.csv\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#training_set = training_set.drop(columns=['Unnamed: 0'])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#test_set = test_set.drop(columns=['Unnamed: 0'])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    676\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 932\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    933\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    934\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1214\u001b[0m             \u001b[1;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[1;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1216\u001b[1;33m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[0;32m   1217\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1218\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 786\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    787\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'online_shoppers_intention.csv'"
     ]
    }
   ],
   "source": [
    "training_set = pd.read_csv(\"online_shoppers_intention.csv\")\n",
    "\n",
    "#test_set = pd.read_csv(\"test_set_complete.csv\")\n",
    "#training_set = training_set.drop(columns=['Unnamed: 0'])\n",
    "#test_set = test_set.drop(columns=['Unnamed: 0'])\n",
    "#training_set['Revenue'] = training_set['Revenue'].astype(int)\n",
    "\n",
    "# DA TOGLIERE UNA VOLTA CHE UNIAMO I NOTEBOOK!!!!\n",
    "\n",
    "training_set['Month']=training_set['Month'].astype('category')\n",
    "training_set['OperatingSystems']=training_set['OperatingSystems'].astype('category')\n",
    "training_set['Browser']=training_set['Browser'].astype('category')\n",
    "training_set['Region']=training_set['Region'].astype('category')\n",
    "training_set['TrafficType']=training_set['TrafficType'].astype('category')\n",
    "training_set['VisitorType']=training_set['VisitorType'].astype('category')\n",
    "training_set['Weekend']=training_set['Weekend'].astype('category')\n",
    "#training_set['Revenue']=training_set['Revenue'].astype('category')\n",
    "\n",
    "\n",
    "# We uniform the categories between training and test set\n",
    "\n",
    "all_categories_browser = list(range(1,14))\n",
    "all_categories_traffic_type = list(range(1,21))\n",
    "training_set['Browser'] = training_set['Browser'].cat.set_categories(all_categories_browser)\n",
    "training_set['TrafficType'] = training_set['TrafficType'].cat.set_categories(all_categories_traffic_type)\n",
    "#training_set['Weekend'] = np.where(training_set['Weekend']=='True',1,0)\n",
    "\n",
    "training_set = pd.get_dummies(training_set, columns=['Month', 'OperatingSystems', 'Browser', 'Region', 'TrafficType', 'VisitorType','Weekend'], drop_first=False)\n",
    "\n",
    "\n",
    "\n",
    "#test_set['Revenue'] = test_set['Revenue'].astype(int)\n",
    "#training_set['Revenue'].value_counts()\n",
    "#test_set['Revenue'].value_counts()\n",
    "categorical_features = ['Month_Aug', 'Month_Dec', 'Month_Feb', 'Month_Jul', 'Month_June',\n",
    "       'Month_Mar', 'Month_May', 'Month_Nov', 'Month_Oct', 'Month_Sep',\n",
    "       'OperatingSystems_1', 'OperatingSystems_2', 'OperatingSystems_3',\n",
    "       'OperatingSystems_4', 'OperatingSystems_5', 'OperatingSystems_6',\n",
    "       'OperatingSystems_7', 'OperatingSystems_8', 'Browser_1', 'Browser_2',\n",
    "       'Browser_3', 'Browser_4', 'Browser_5', 'Browser_6', 'Browser_7',\n",
    "       'Browser_8', 'Browser_9', 'Browser_10', 'Browser_11', 'Browser_12',\n",
    "       'Browser_13', 'Region_1', 'Region_2', 'Region_3', 'Region_4',\n",
    "       'Region_5', 'Region_6', 'Region_7', 'Region_8', 'Region_9',\n",
    "       'TrafficType_1', 'TrafficType_2', 'TrafficType_3', 'TrafficType_4',\n",
    "       'TrafficType_5', 'TrafficType_6', 'TrafficType_7', 'TrafficType_8',\n",
    "       'TrafficType_9', 'TrafficType_10', 'TrafficType_11', 'TrafficType_12',\n",
    "       'TrafficType_13', 'TrafficType_14', 'TrafficType_15', 'TrafficType_16',\n",
    "       'TrafficType_17', 'TrafficType_18', 'TrafficType_19', 'TrafficType_20',\n",
    "       'VisitorType_New_Visitor', 'VisitorType_Other',\n",
    "       'VisitorType_Returning_Visitor', 'Weekend_False', 'Weekend_True']\n",
    "#training_set[categorical_features] = training_set[categorical_features].astype('category')\n",
    "#test_set[categorical_features] = test_set[categorical_features].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8acdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full = training_set.drop(columns=['Revenue'])\n",
    "#X_test_full = test_set.drop(columns=['Revenue'])\n",
    "y_train = training_set['Revenue']\n",
    "#y_test = test_set['Revenue']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ef0717",
   "metadata": {},
   "source": [
    "### Variance selection\n",
    "We apply variance selection to remove all the features with a very low variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4622c48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "threshold = 0.001  # Soglia di varianza desiderata\n",
    "variance_selector = VarianceThreshold(threshold=threshold)\n",
    "\n",
    "# Applicazione della selezione della varianza sul dataset\n",
    "\n",
    "X_train = X_train_full.loc[:, variance_selector.fit(X_train_full).get_support()]\n",
    "columns_remaining = X_train.columns.tolist()\n",
    "\n",
    "#X_test = X_test_full[columns_remaining]\n",
    "print(X_train.columns.tolist())\n",
    "len(columns_remaining)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e2ae20",
   "metadata": {},
   "source": [
    "### Functions and algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfbd150",
   "metadata": {},
   "source": [
    "#### Feature selection: Forward stepwise selection\n",
    "We chose this algorithm because it's more scalable on a big dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f488230e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "def get_evaluator(scorer):\n",
    "    def evaluator(model, X, y, trained=False):\n",
    "        if not trained:\n",
    "            model = model.fit(X, y)\n",
    "        score = scorer(model, X, y)\n",
    "        return model, score\n",
    "    return evaluator   \n",
    "\n",
    "def get_cv_evaluator(scorer, cv=3):\n",
    "    def evaluator(model, X, y, trained=False):            \n",
    "        scores = cross_val_score(model, X, y, scoring=scorer, cv=cv)\n",
    "        if not trained:\n",
    "            model = model.fit(X, y)\n",
    "        return model, np.mean(scores)\n",
    "    \n",
    "    return evaluator\n",
    "\n",
    "def get_val_evaluator(scorer, val_size=0.1):\n",
    "    def evaluator(model, X, y, trained=False):\n",
    "        X_train_small, X_val, y_train_small, y_val = train_test_split(X, y, \n",
    "                                                                      test_size=val_size,\n",
    "                                                                      random_state=0)\n",
    "        \n",
    "        if not trained:\n",
    "            model = model.fit(X_train_small, y_train_small)\n",
    "        score = scorer(model, X_val, y_val) \n",
    "        \n",
    "        return model, score\n",
    "    \n",
    "    return evaluator\n",
    "\n",
    "\n",
    "possible_models = {\n",
    "    \"LogisticRegression\":LogisticRegression(solver=\"newton-cg\", max_iter=1000),\n",
    "    \"LDA\":LDA(),\n",
    "    \"QDA\":QDA(),\n",
    "    #\"KNN\":KNeighborsClassifier(n_neighbors=6),\n",
    "    \"LinearRegression\":LinearRegression(fit_intercept=True),\n",
    "    \"SVM\":LinearSVC(C=0.1,max_iter=1000)\n",
    "}\n",
    "\n",
    "#da qui inizia il ctrl Z\n",
    "\n",
    "def forward_selection(Xtrain_pd, ytrain, model_chosen,\n",
    "                      candidates_evaluator, candidates_argbest, # Metric to be used at 2.b\n",
    "                      subsets_evaluator, subsets_argbest,       # Metric to be used at 3\n",
    "                      candidates_scorer_name=None,  # Name of 2. figure\n",
    "                      subsets_scorer_name=None,     # Name of 3. figure\n",
    "                      verbose=True, weight_step3=0):   \n",
    "    \n",
    "    \n",
    "    # Global variable init\n",
    "    # ====================\n",
    "    num_features = Xtrain_pd.shape[-1]\n",
    "    best_candidate_metric = []\n",
    "    # subsets_* are lists containing one value for each Mk model (the best of the Mk candidates)\n",
    "    subsets_metric = []        # The best metric of each subset of dimension 'dim'\n",
    "    subsets_best_features = [] # The best features combination in each subset of dimension 'dim'\n",
    "    # A figure to keep track of candidates scores in each Mk subset\n",
    "    num_evaluations = 0        # A conter to keep track of the total number of trials\n",
    "    \n",
    "    selected_features = []\n",
    "    all_features = Xtrain_pd.columns\n",
    "    \n",
    "    \n",
    "    # 1. Train M0\n",
    "    # ===========\n",
    "    model = DummyRegressor()\n",
    "    # Compute (2.b) metrics\n",
    "    model, score = candidates_evaluator(model, Xtrain_pd[[]], ytrain)\n",
    "    best_candidate_metric.append(score)\n",
    "    subsets_best_features.append([])\n",
    "    # Compute metric for step 3.\n",
    "    _, score = subsets_evaluator(model, Xtrain_pd[[]], ytrain, trained=True)\n",
    "    subsets_metric.append(score)\n",
    "    \n",
    "    \n",
    "    for dim in range(num_features):\n",
    "        candidate_metrics = [] # metrics for all the models with dim features\n",
    "        candidate_models = []  # models with dim features\n",
    "        \n",
    "        remaining_features = all_features.difference(selected_features)\n",
    "        \n",
    "        # fit all the models with k features\n",
    "        for new_column in remaining_features:\n",
    "            Xtrain_sub = Xtrain_pd[selected_features+[new_column]].to_numpy()\n",
    "            model = possible_models[model_chosen]\n",
    "            #print(new_column)\n",
    "            model, score = candidates_evaluator(model, Xtrain_sub, ytrain)\n",
    "            candidate_models.append(model)\n",
    "            candidate_metrics.append(score)\n",
    "            num_evaluations += 1\n",
    "            \n",
    "        \n",
    "        idx_best_candidate = candidates_argbest(candidate_metrics) # select the best Mk model\n",
    "        selected_features.append(remaining_features[idx_best_candidate]) # Update selected feature\n",
    "        best_candidate_metric.append(candidate_metrics[idx_best_candidate]) # Save best candidate features\n",
    "        best_features = selected_features.copy()\n",
    "        subsets_best_features.append(best_features)\n",
    "        \n",
    "        \n",
    "        # Compute metric for the final step -> comparison of all the best models\n",
    "        best_subset_model = candidate_models[idx_best_candidate] # save the best model\n",
    "        best_subset_Xtrain = Xtrain_pd[best_features].to_numpy()\n",
    "        _, score = subsets_evaluator(best_subset_model, best_subset_Xtrain, ytrain, trained=True)\n",
    "        subsets_metric.append(score) #computing the metrics for the training set\n",
    "        num_evaluations += weight_step3 \n",
    "        \n",
    "        if verbose:\n",
    "            print(\"............\")\n",
    "            print(\"Best model (M{}) with {} features: {}\".format(dim+1, dim+1, best_features))\n",
    "            print(\"M{} subset score (3.): {}\".format(dim+1, score))\n",
    "        \n",
    "    # choose the best candidates\n",
    "    best_subset_idx = subsets_argbest(subsets_metric)\n",
    "    best_features = subsets_best_features[best_subset_idx]\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\n Best configuration has {} features\".format(best_subset_idx))\n",
    "        print(\"Features: {}\".format(subsets_best_features[best_subset_idx]))\n",
    "        print(\"Total number of trained models:\", num_evaluations)\n",
    "    \n",
    "    # Complete the subsets_fig figure by plotting\n",
    "    # a line connecting all best candidate score\n",
    "    best_candidate_score_idx = candidates_argbest(best_candidate_metric)\n",
    "    return best_features\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06c7e91",
   "metadata": {},
   "source": [
    "#### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd571b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_pred, y_true):\n",
    "    return (y_pred == y_true).mean()\n",
    "\n",
    "def calculate_sensitivity_specificity(confusion_matrix):\n",
    "    # Extract values from the confusion matrix\n",
    "    TN, FP, FN, TP = confusion_matrix.ravel()\n",
    "\n",
    "    # Calculate Sensitivity (Recall)\n",
    "    sensitivity = TP / (TP + FN)\n",
    "\n",
    "    # Calculate Specificity\n",
    "    specificity = TN / (TN + FP)\n",
    "\n",
    "    return sensitivity, specificity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982a7418",
   "metadata": {},
   "source": [
    "#### Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160e03e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def classification_metrics_training(model, model_name, X_train, y_train, cv):\n",
    "    #sensitivity\n",
    "    sensitivity_train = np.mean(np.absolute(cross_val_score(model, X_train, y_train, cv=cv, scoring='recall', n_jobs=-1)))\n",
    "    \n",
    "    #specificity\n",
    "    specificity_train = np.mean(np.absolute(cross_val_score(model, X_train, y_train, cv=cv, scoring='precision', n_jobs=-1)))\n",
    "    \n",
    "    #accuracy\n",
    "    accuracy_train = np.mean(np.absolute(cross_val_score(model, X_train, y_train, cv=cv, scoring='accuracy', n_jobs=-1)))\n",
    "                         \n",
    "    # building a dataframe\n",
    "    data = {\n",
    "        'Model': [model_name],\n",
    "        'Accuracy': [accuracy_train],\n",
    "        'Sensitivity': [sensitivity_train],\n",
    "        'Specificity': [specificity_train],\n",
    "    }\n",
    "    return pd.DataFrame(data)                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e866b1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_metrics_test(model, model_name,X_train,y_train,X_test,y_test,threshold):\n",
    "    # building the confusion matrix for test performances\n",
    "    model = model.fit(X_train,y_train)\n",
    "    #ypred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]  # Probability of positive class (class 1)\n",
    "\n",
    "    # Convert probabilities to binary predictions\n",
    "    ypred = (y_pred_proba > threshold).astype(int)\n",
    "    \n",
    "    cm = confusion_matrix(y_test.to_numpy(), np.array(ypred))\n",
    "    sensitivity_test, specificity_test = calculate_sensitivity_specificity(cm)\n",
    "    \n",
    "    accuracy_test = accuracy(ypred, y_test)\n",
    "    \n",
    "    # building a dataframe\n",
    "    data = {\n",
    "        'Model': [model_name],\n",
    "        'Accuracy': [accuracy_test],\n",
    "        'Sensitivity': [sensitivity_test],\n",
    "        'Specificity': [specificity_test],\n",
    "    }\n",
    "    return pd.DataFrame(data), cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd019a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We compute the empirical threshold\n",
    "empirical_threshold = y_train.mean()\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "def classification_metrics_empirical_threshold(model, model_name, X_train, y_train,threshold=0.5):\n",
    "    \n",
    "    model = model.fit(X_train, y_train)\n",
    "    \n",
    "    # Get predicted probabilities\n",
    "    y_pred_proba = model.predict_proba(X_train)[:, 1]  # Probability of positive class (class 1)\n",
    "\n",
    "    # Convert probabilities to binary predictions\n",
    "    y_pred = (y_pred_proba > threshold).astype(int)\n",
    "\n",
    "    # Evaluate the performance using various metrics\n",
    "    accuracy = accuracy_score(y_train, y_pred)\n",
    "    specificity = precision_score(y_train, y_pred)\n",
    "    sensitivity = recall_score(y_train, y_pred)\n",
    "    f1 = f1_score(y_train, y_pred)\n",
    "    \n",
    "    data = {\n",
    "        'Model': [model_name],\n",
    "        'Accuracy': [accuracy],\n",
    "        'Sensitivity': [sensitivity],\n",
    "        'Specificity': [specificity],\n",
    "    }\n",
    "    return pd.DataFrame(data) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12901cd6",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792f64c9",
   "metadata": {},
   "source": [
    "### Scaling data\n",
    "We scale the numerical features to avoid convergence problem with logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fd591a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FACCIAMO QUESTO STEP PER EVITARE PROBLEMI DI CONVERGENZA NEL MODELLO\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assuming you have your feature data X\n",
    "numeric_features = ['Administrative', 'Administrative_Duration', 'Informational',\n",
    "       'Informational_Duration', 'ProductRelated', 'ProductRelated_Duration',\n",
    "       'BounceRates', 'ExitRates', 'PageValues', 'SpecialDay']\n",
    "categorical_features = ['Month_Aug', 'Month_Dec', 'Month_Feb', 'Month_Jul', 'Month_June', 'Month_Mar', 'Month_May', \n",
    "                        'Month_Nov', 'Month_Oct', 'Month_Sep', 'OperatingSystems_1', 'OperatingSystems_2', 'OperatingSystems_3', 'OperatingSystems_4', 'OperatingSystems_6', 'OperatingSystems_8', 'Browser_1', 'Browser_2', 'Browser_3', 'Browser_4', 'Browser_5', 'Browser_6', 'Browser_7', 'Browser_8', 'Browser_10', 'Browser_13', 'Region_1', 'Region_2', 'Region_3', 'Region_4', 'Region_5',\n",
    "                        'Region_6', 'Region_7', 'Region_8', 'Region_9', 'TrafficType_1', 'TrafficType_2', \n",
    "                        'TrafficType_3', 'TrafficType_4', 'TrafficType_5', 'TrafficType_6', 'TrafficType_7', \n",
    "                        'TrafficType_8', 'TrafficType_9', 'TrafficType_10', 'TrafficType_11', 'TrafficType_13', \n",
    "                        'TrafficType_15', 'TrafficType_19', 'TrafficType_20', 'VisitorType_New_Visitor', 'VisitorType_Other', 'VisitorType_Returning_Visitor', 'Weekend_False', 'Weekend_True']\n",
    "scaler = StandardScaler()\n",
    "train_scaled = pd.DataFrame(scaler.fit_transform(X_train[numeric_features]))\n",
    "#test_scaled = pd.DataFrame(scaler.fit_transform(X_test[numeric_features]))\n",
    "train_scaled.columns = X_train[numeric_features].columns\n",
    "#test_scaled.columns = X_test[numeric_features].columns\n",
    "X_train_scaled = pd.concat([train_scaled,X_train[categorical_features]],axis=1)\n",
    "#X_test_scaled = pd.concat([test_scaled,X_test[categorical_features]],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687db0bf",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535ef449",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=5, random_state=42, shuffle=True)\n",
    "features_logistic = forward_selection(X_train_scaled, y_train, \"LogisticRegression\",\n",
    "                  get_evaluator(make_scorer(accuracy)), np.argmax, # 2.\n",
    "                  get_cv_evaluator(make_scorer(accuracy), cv), np.argmax, # 3.\n",
    "                  candidates_scorer_name=\"Accuracy\",\n",
    "                  subsets_scorer_name=\"Accuracy (CV)\",\n",
    "                  verbose=True, weight_step3=cv.n_splits)\n",
    "features_logistic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efe8489",
   "metadata": {},
   "source": [
    "### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc10a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_logistic = LogisticRegression(solver=\"newton-cg\",penalty=\"none\",max_iter=1000)\n",
    "train_scores_logistic = classification_metrics_training(model_logistic, \"LogisticRegression\", X_train_scaled[features_logistic], y_train,cv)\n",
    "train_scores_logistic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec82040",
   "metadata": {},
   "source": [
    "The sensitivity is very low, we try to improve it using the empirical threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf18bb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores_logistic_empirical = classification_metrics_empirical_threshold(model_logistic, 'LogisticRegression', X_train_scaled[features_logistic], y_train,empirical_threshold)\n",
    "train_scores_logistic_empirical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bd991c",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc563bb6",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d04943",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=5, random_state=42, shuffle=True)\n",
    "features_LDA = forward_selection(X_train_scaled, y_train, \"LDA\",\n",
    "                  get_evaluator(make_scorer(accuracy)), np.argmax, # 2.\n",
    "                  get_cv_evaluator(make_scorer(accuracy), cv), np.argmax, # 3.\n",
    "                  candidates_scorer_name=\"Accuracy\",\n",
    "                  subsets_scorer_name=\"Accuracy (CV)\",\n",
    "                  verbose=True, weight_step3=cv.n_splits)\n",
    "features_LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a043ba09",
   "metadata": {},
   "source": [
    "### Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc267b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_LDA =LDA(store_covariance=True)\n",
    "train_scores_LDA = classification_metrics_training(model_LDA, \"LDA\", X_train_scaled[features_LDA], y_train, cv)\n",
    "train_scores_LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b16029",
   "metadata": {},
   "source": [
    "We try to improve the sensitivity by setting the empirical threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e3cb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores_LDA_empirical = classification_metrics_empirical_threshold(model_LDA, 'LDA', X_train_scaled[features_LDA], y_train,empirical_threshold)\n",
    "train_scores_LDA_empirical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0d07a9",
   "metadata": {},
   "source": [
    "## QDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6265074c",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee0e365",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import warnings\n",
    "#warnings.filterwarnings(\"ignore\")\n",
    "cv = KFold(n_splits=5, random_state=42, shuffle=True)\n",
    "features_QDA = forward_selection(X_train_scaled, y_train, \"QDA\",\n",
    "                  get_evaluator(make_scorer(accuracy)), np.argmax, # 2.\n",
    "                  get_cv_evaluator(make_scorer(accuracy), cv), np.argmax, # 3.\n",
    "                  candidates_scorer_name=\"Accuracy\",\n",
    "                  subsets_scorer_name=\"Accuracy (CV)\",\n",
    "                  verbose=True, weight_step3=cv.n_splits)\n",
    "features_QDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7033b22a",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb46676",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_QDA = QDA(store_covariance=True)\n",
    "train_scores_QDA = classification_metrics_training(model_QDA, \"QDA\", X_train_scaled[features_QDA], y_train, cv)\n",
    "train_scores_QDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723e77d7",
   "metadata": {},
   "source": [
    "The sensitivity is very high with respect to the specificity, so we try to set the empirical threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f41197",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores_QDA_empirical = classification_metrics_empirical_threshold(model_QDA, 'QDA', X_train_scaled[features_QDA], y_train, empirical_threshold)\n",
    "train_scores_QDA_empirical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91233d5",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a20dd3a",
   "metadata": {},
   "source": [
    "### With all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3215791",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92da8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_KNN_all = KNeighborsClassifier()\n",
    "params = {'n_neighbors': range(1, 15)}\n",
    "cv = KFold(n_splits=5, random_state=42, shuffle=True)\n",
    "cv_KNN_all = GridSearchCV(model_KNN_all, params, refit=True, cv=cv, \n",
    "                  scoring=make_scorer(accuracy))\n",
    "cv_KNN_all.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a2d1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_KNN_all = KNeighborsClassifier(n_neighbors=cv_KNN_all.best_params_['n_neighbors'])\n",
    "train_scores_KNN_all = classification_metrics_training(model_KNN_all, \"KNN\", X_train, y_train, cv)\n",
    "train_scores_KNN_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a86f80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores_KNN_all_empirical = classification_metrics_empirical_threshold(model_KNN_all, 'KNN', X_train, y_train,empirical_threshold)\n",
    "train_scores_KNN_all_empirical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc0b4fc",
   "metadata": {},
   "source": [
    "### With only numeric features\n",
    "KNN has two main issues: \n",
    "1. It is not scalable with a large amount of features\n",
    "2. It relies on a distance metric to compute the distance between points. This distance may not be significant in case of categorical variables (even if we use one-hot encoding)\n",
    "3. From the previous analysis we can also notice that even with the empirical threshold the performances don't improve significantly\n",
    "\n",
    "So for these reasons we also fit a model with just the numeric features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00843bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_KNN_numeric = KNeighborsClassifier()\n",
    "params = {'n_neighbors': range(1, 50)}\n",
    "cv_KNN_numeric = GridSearchCV(model_KNN_numeric, params, refit=True, cv=cv, \n",
    "                  scoring=make_scorer(accuracy))\n",
    "cv_KNN_numeric.fit(X_train[numeric_features], y_train)\n",
    "model_KNN_numeric = KNeighborsClassifier(n_neighbors=cv_KNN_numeric.best_params_['n_neighbors'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9984ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores_KNN_numeric = classification_metrics_training(model_KNN_numeric, \"KNN\", X_train[numeric_features], y_train, cv)\n",
    "train_scores_KNN_numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0611f3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores_KNN_numeric_empirical = classification_metrics_empirical_threshold(model_KNN_numeric, 'KNN', X_train[numeric_features], y_train, empirical_threshold)\n",
    "train_scores_KNN_numeric_empirical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7590e1",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab17648",
   "metadata": {},
   "source": [
    "### Choosing the optimal kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36026f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels = ['linear', 'rbf', 'poly', 'sigmoid']\n",
    "score = []\n",
    "for kern in kernels:\n",
    "    cv = KFold(n_splits=5, random_state=1, shuffle=True)\n",
    "    print(kern)\n",
    "    if kern == 'linear':\n",
    "        print('sono linear...')\n",
    "        model_SVM = LinearSVC()\n",
    "    else:\n",
    "        print('nonlinear')\n",
    "        model_SVM = SVC(kernel=kern)\n",
    "    scores = cross_val_score(model_SVM, X_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "    score.append(np.mean(scores))\n",
    "\n",
    "max_index = score.index(max(score))\n",
    "best_kernel = kernels[max_index]\n",
    "acc_best_model = max(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5352c4c6",
   "metadata": {},
   "source": [
    "### Choosing the best C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a910fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "C_s= [0.1,1,10,100]\n",
    "\n",
    "score_c = []\n",
    "   \n",
    "for c in C_s:\n",
    "    model_SVM = LinearSVC(C=c)\n",
    "    cv = KFold(n_splits=5, random_state=1, shuffle=True)\n",
    "    scores_score = cross_val_score(model_SVM, X_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "    score_c.append(np.mean(scores_score))\n",
    "best_score_c = score_c.index(max(score_c))\n",
    "   \n",
    "\n",
    "best_c = C_s[best_score_c]\n",
    "best_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0f0f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "#warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"default\")\n",
    "\n",
    "cv = KFold(n_splits=5, random_state=42, shuffle=True)\n",
    "features_SVM = forward_selection(X_train_scaled, y_train, \"SVM\",\n",
    "                  get_evaluator(make_scorer(accuracy)), np.argmax, # 2.\n",
    "                  get_cv_evaluator(make_scorer(accuracy), cv), np.argmax, # 3.\n",
    "                  candidates_scorer_name=\"Accuracy\",\n",
    "                  subsets_scorer_name=\"Accuracy (CV)\",\n",
    "                  verbose=True, weight_step3=cv.n_splits)\n",
    "features_SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349158e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_SVM = LinearSVC()\n",
    "model_SVM = model_SVM.fit(X_train_scaled[features_SVM],y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62db157d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores_SVM = classification_metrics_training(model_SVM, \"SVM\", X_train_scaled[features_SVM], y_train, cv)\n",
    "train_scores_SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd05b05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_scores_SVM_empirical = classification_metrics_empirical_threshold(model_SVM, 'SVM', X_train_scaled[features_SVM], y_train, empirical_threshold)\n",
    "#train_scores_SVM_empirical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fff5c13",
   "metadata": {},
   "source": [
    "## Algorithm comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8418f866",
   "metadata": {},
   "source": [
    "### Metric comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d769180a",
   "metadata": {},
   "source": [
    "#### Threshold = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9660726d",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = pd.concat([train_scores_logistic,train_scores_LDA,train_scores_QDA,train_scores_KNN_all,train_scores_KNN_numeric,train_scores_SVM],axis=0)\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611b6d11",
   "metadata": {},
   "source": [
    "#### Empirical threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60add0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df_empirical = pd.concat([train_scores_logistic_empirical,train_scores_LDA_empirical,train_scores_QDA_empirical,train_scores_KNN_all_empirical,train_scores_KNN_numeric_empirical],axis=0)\n",
    "metrics_df_empirical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ae2a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "def plot_roc(predict_fn, X, y, label=None):\n",
    "    fprs, tprs, t = roc_curve(y, predict_fn(X)[:,-1])\n",
    "    \n",
    "    # Plot the ROC\n",
    "    plt.plot(fprs, tprs, label=\"ROC \"+label)\n",
    "    plt.xlabel(\"FPR = 1 - specificity\")\n",
    "    plt.ylabel(\"TPR = sensitivity\")\n",
    "    plt.legend()\n",
    "\n",
    "auc_list = []\n",
    "\n",
    "for name, model, X in [('lr', model_logistic, X_train_scaled[features_logistic]), \n",
    "                    ('lda', model_LDA, X_train_scaled[features_LDA]), \n",
    "                    ('qda', model_QDA, X_train_scaled[features_QDA]),\n",
    "                    #('SVM', model_SVM,X_train_scaled[features_SVM]),\n",
    "                    ('KNN-all', model_KNN_all, X_train),\n",
    "                    ('KNN-numeric', model_KNN_numeric, X_train[numeric_features])\n",
    "                   ]:\n",
    "    model.fit(X, y_train) # sistemare questa parte, fitta i modelli prima in modo che sia tutto ok\n",
    "    plot_roc(model.predict_proba, X, y_train, name)\n",
    "    y_prob_pred = model.predict_proba(X)[:, 1]\n",
    "    auc = roc_auc_score(y_train, y_prob_pred)\n",
    "    auc_list.append(auc)\n",
    "auc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055f4667",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
